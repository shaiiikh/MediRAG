{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11248710,"sourceType":"datasetVersion","datasetId":7028869}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Knowledge Graphs (diagnostic_kg) → JSON files containing diagnostic pathways and medical knowledge.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import json\nimport os\nkg_path = \"/kaggle/input/medical-dataset/diagnostic_kg/Diagnosis_flowchart\"\nknowledge_graphs = {}\nfor file in os.listdir(kg_path):\n    if file.endswith(\".json\"):\n        with open(os.path.join(kg_path, file), \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        diagnostic_steps = data.get(\"diagnostic\", {})\n        knowledge_info = data.get(\"knowledge\", {})\n        knowledge_text = \"\"\n        for step, details in knowledge_info.items():\n            if isinstance(details, dict):  \n                for key, value in details.items():\n                    knowledge_text += f\"{step} - {key}: {value}\\n\"\n            else:  \n                knowledge_text += f\"{step}: {details}\\n\"\n        knowledge_graphs[file] = knowledge_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:51:03.596969Z","iopub.execute_input":"2025-04-03T15:51:03.597274Z","iopub.status.idle":"2025-04-03T15:51:03.790885Z","shell.execute_reply.started":"2025-04-03T15:51:03.597250Z","shell.execute_reply":"2025-04-03T15:51:03.790282Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Annotated Clinical Notes (samples) → JSON files with real patient records and step-by-step diagnoses.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\n# Path to clinical notes\nsample_path = \"/kaggle/input/medical-dataset/samples/Finished\"\n\n# Dictionary to store clinical notes\nclinical_notes = {}\n\n# Loop through disease categories inside 'Finished'\nfor disease_category in os.listdir(sample_path):\n    disease_path = os.path.join(sample_path, disease_category)\n    \n    if os.path.isdir(disease_path):  # Ensure it's a folder\n        clinical_notes[disease_category] = {}\n        \n        # Loop through subcategories inside each disease category\n        for subcategory in os.listdir(disease_path):\n            subcategory_path = os.path.join(disease_path, subcategory)\n            \n            if os.path.isdir(subcategory_path):  # Ensure it's a folder\n                clinical_notes[disease_category][subcategory] = []\n                \n                # Traverse JSON files inside subcategory\n                for file in os.listdir(subcategory_path):\n                    if file.endswith(\".json\"):\n                        file_path = os.path.join(subcategory_path, file)\n                        \n                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                            data = json.load(f)\n                        \n                        # Extract useful fields\n                        note_text = \"\"\n                        for key, value in data.items():\n                            note_text += f\"{key}: {value}\\n\"\n                        \n                        # Store extracted text\n                        clinical_notes[disease_category][subcategory].append(note_text)\n\n# Now 'clinical_notes' contains structured data organized by disease category and subcategory\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:51:03.792007Z","iopub.execute_input":"2025-04-03T15:51:03.792297Z","iopub.status.idle":"2025-04-03T15:51:07.739252Z","shell.execute_reply.started":"2025-04-03T15:51:03.792266Z","shell.execute_reply":"2025-04-03T15:51:07.738617Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Clean and Preprocess the Text","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    \"\"\"\n    Cleans and normalizes medical text.\n    - Removes special characters\n    - Normalizes spacing\n    - Converts lists into readable format\n    \"\"\"\n    text = re.sub(r'\\n+', '\\n', text)  # Remove extra new lines\n    text = re.sub(r'[^\\w\\s.;,]', '', text)  # Remove special characters\n    text = text.strip()\n    return text\n\ndef clean_nested_data(data):\n    \"\"\"\n    Recursively applies cleaning to nested data (text content).\n    \"\"\"\n    if isinstance(data, str):  # If the data is a string, clean it\n        return clean_text(data)\n    \n    if isinstance(data, dict):  # If the data is a dictionary, apply cleaning recursively\n        return {key: clean_nested_data(value) for key, value in data.items()}\n    \n    if isinstance(data, list):  # If the data is a list, apply cleaning recursively to each item\n        return [clean_nested_data(item) for item in data]\n    \n    return data  # If it's neither string, dict, nor list, return as is\n\n# Apply cleaning to both knowledge graphs and clinical notes\nknowledge_graphs = {key: clean_text(value) for key, value in knowledge_graphs.items()}\nclinical_notes = clean_nested_data(clinical_notes)  # Apply the recursive cleaning for nested structure\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:51:07.740569Z","iopub.execute_input":"2025-04-03T15:51:07.740779Z","iopub.status.idle":"2025-04-03T15:51:07.825063Z","shell.execute_reply.started":"2025-04-03T15:51:07.740761Z","shell.execute_reply":"2025-04-03T15:51:07.824548Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Graph to Document (Format the Data for Retrieval)","metadata":{}},{"cell_type":"code","source":"from langchain.docstore.document import Document\n\n# Prepare documents for retrieval\ndocuments = []\n\n# Convert Knowledge Graphs\nfor file, content in knowledge_graphs.items():\n    documents.append(Document(page_content=content, metadata={\"source\": file, \"type\": \"knowledge_graph\"}))\n\n# Convert Clinical Notes (Handling Nested Structure)\nfor disease_category, subcategories in clinical_notes.items():\n    for subcategory, notes in subcategories.items():\n        for idx, note in enumerate(notes):  # Each note is a separate document\n            documents.append(Document(\n                page_content=note,\n                metadata={\n                    \"source\": f\"{disease_category}/{subcategory}/note_{idx+1}\",\n                    \"type\": \"clinical_note\",\n                    \"disease_category\": disease_category,\n                    \"subcategory\": subcategory\n                }\n            ))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:51:07.826161Z","iopub.execute_input":"2025-04-03T15:51:07.826435Z","iopub.status.idle":"2025-04-03T15:51:09.025821Z","shell.execute_reply.started":"2025-04-03T15:51:07.826381Z","shell.execute_reply":"2025-04-03T15:51:09.025180Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Storeing the Data in a Vector Database","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu langchain sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:51:09.026520Z","iopub.execute_input":"2025-04-03T15:51:09.026724Z","iopub.status.idle":"2025-04-03T15:51:15.019077Z","shell.execute_reply.started":"2025-04-03T15:51:09.026704Z","shell.execute_reply":"2025-04-03T15:51:15.018197Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.12)\nCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\nRequirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.11.0a2)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.29.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\nDownloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nInstalling collected packages: async-timeout, faiss-cpu\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed async-timeout-4.0.3 faiss-cpu-1.10.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install -U langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:51:15.020021Z","iopub.execute_input":"2025-04-03T15:51:15.020313Z","execution_failed":"2025-04-03T16:32:46.720Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain-community\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain-core<1.0.0,>=0.3.45 (from langchain-community)\n  Downloading langchain_core-0.3.50-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain<1.0.0,>=0.3.21 (from langchain-community)\n  Downloading langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain<1.0.0,>=0.3.21->langchain-community)\n  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.0a2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.29.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain-community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\nDownloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.22-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.50-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.4/423.4 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub\n!pip install --upgrade sentence-transformers\n!pip install --upgrade bitsandbytes\n!pip install --upgrade transformers accelerate\n!pip install --upgrade transformers bitsandbytes accelerate sentence-transformers\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers bitsandbytes accelerate sentence-transformers","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain.embeddings import HuggingFaceEmbeddings\n\n# # Load sentence transformer model\n# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom sentence_transformers import SentenceTransformer\n\n# Verify CUDA and GPU\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n\n# Configure 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True\n)\n\n# Load the model manually with quantization\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n\n# Wrap it into SentenceTransformer\nembedding_model = SentenceTransformer(model_name)\nembedding_model._first_module().auto_model = model  # Inject the quantized model\n\n# Initialize embeddings\nembedding = HuggingFaceEmbeddings(model_name=model_name)\n\nprint(\"Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.723Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Convert Documents into Embeddings","metadata":{}},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore.document import Document\n\n# Initialize the embedding model with BERT base uncased\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # 384-dim model\n\n# Convert dictionaries to LangChain Documents\ndocuments = []\n\n# Knowledge Graphs\nfor file, content in knowledge_graphs.items():\n    documents.append(Document(page_content=content, metadata={\"source\": file, \"type\": \"knowledge_graph\"}))\n\n# Clinical Notes\nfor disease, subcategories in clinical_notes.items():\n    for subcategory, notes in subcategories.items():\n        for note in notes:\n            documents.append(Document(page_content=note, metadata={\"source\": disease, \"subcategory\": subcategory, \"type\": \"clinical_note\"}))\nfaiss_index = FAISS.from_documents(documents, embedding_model)\nfaiss_index.save_local(\"faiss_index\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vectorstore = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implementing the top-k simimlarty to get the results","metadata":{}},{"cell_type":"markdown","source":"## Load the FAISS Vector Store","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade langchain\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom transformers import pipeline","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvectorstore = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# os.environ['TRANSFORMERS_CACHE'] = '/path/to/cache/directory'\n# os.environ['BITSANDBYTES_CACHING'] = '1'","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\nimport bitsandbytes as bnb\n\n# Verify CUDA is available\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"BitsAndBytes version: {bnb.__version__}\")\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nif torch.cuda.is_available():\n    # GPU configuration with 4-bit quantization\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        trust_remote_code=True\n    )\nelse:\n    # If no GPU, try loading with 8-bit quantization or regular loading\n    print(\"No GPU detected. Loading model with basic configuration...\")\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=\"auto\",\n            torch_dtype=torch.float32,\n            trust_remote_code=True\n        )\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        # If that fails, try with a smaller model\n        print(\"Consider using a smaller model or enabling GPU runtime\")\n        raise\n\n# Load pipeline\nqa_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def answer_clinical_query(query):\n    docs = retriever.invoke(query)  # Fix deprecated method\n    context = \"\\n\".join([doc.page_content for doc in docs])\n    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n    \n    # Fix `max_length` error\n    response = qa_pipeline(prompt, max_new_tokens=256, num_return_sequences=1)\n    \n    return response[0]['generated_text']\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"what is CT scan\"\nresponse = answer_clinical_query(query)\nprint(response)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain.embeddings import HuggingFaceEmbeddings\n# from langchain.vectorstores import FAISS\n# from langchain.docstore.document import Document\n# from transformers import pipeline\n\n# # Initialize the embedding model\n# embedding_model = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")\n\n# # Convert dictionaries to LangChain Documents\n# documents = []\n\n# # Knowledge Graphs\n# for file, content in knowledge_graphs.items():\n#     documents.append(Document(page_content=content, metadata={\"source\": file, \"type\": \"knowledge_graph\"}))\n\n# # Clinical Notes\n# for disease, subcategories in clinical_notes.items():\n#     for subcategory, notes in subcategories.items():\n#         for note in notes:\n#             documents.append(Document(page_content=note, metadata={\"source\": disease, \"subcategory\": subcategory, \"type\": \"clinical_note\"}))\n\n# # Initialize the FAISS Vector Store (make sure the embedding model is the same as the one used for retrieval)\n# faiss_index = FAISS.from_documents(documents, embedding_model)\n\n# # Retrieve the retriever\n# retriever = faiss_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n\n# # Initialize the QA pipeline\n# qa_pipeline = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\", device=0)\n\n# # Function to answer clinical query\n# def answer_clinical_query(query):\n#     # Retrieve relevant documents\n#     docs = retriever.get_relevant_documents(query)\n    \n#     # Combine retrieved knowledge\n#     context = \"\\n\".join([doc.page_content for doc in docs])\n    \n#     # Generate an answer using BERT-based model\n#     prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n#     response = qa_pipeline(prompt, max_length=256, num_return_sequences=1)\n    \n#     return response[0]['generated_text']\n\n# # Test query\n# query = \"What are the symptoms of heart failure?\"\n# response = answer_clinical_query(query)\n# print(response)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-03T16:32:46.727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}